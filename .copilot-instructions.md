# Copilot Instructions — Spec-to-System Mode

Purpose
-------
This document instructs the AI engineer to convert BDD specifications (.feature files and specs/**) into production-grade code, tests, infra, SBOMs, and living documentation while enforcing strict compliance and traceability.

Principles
----------
- Spec-First: Humans supply only BDD `.feature` files and formal requirement documents. All code and tests must be generated from these specs.
- Traceability: Every implemented line of code must map back to a spec scenario or a requirement.
- Hexagonal Architecture: Generated code must follow a clear separation of domain/core logic, ports/adapters, and infrastructure.
- Fail-Fast Compliance: If any checkpoint (spec validation, tests, SAST, resource KPIs) fails in Compliance mode, stop and block merge.

Modes
-----
- Sandbox: Allow exploratory generation and partial implementations. PRs are informational; merges optional.
- Compliance: Strict enforcement. All checkpoints must pass before merge.

High-Level Workflow
-------------------
1. `validate-specs.yml` runs on PRs that touch `specs/**` or `**/*.feature`.
   - Validates Gherkin syntax, ensures Given/When/Then completeness, and checks requirement mapping.
   - Produces `reports/spec-coverage.json` (see Spec Validation output format).

2. `auto-generate.yml` listens to successful spec validation.
   - Generates domain code, adapters, unit & integration tests, and living docs.
   - Commits generated artifacts to an `auto/spec-implementation` branch and opens a PR.

3. `ci.yml` runs on the AI branch / PRs and on merges to `main`.
   - Runs lint, tests, docker build, SBOM generation, SAST (in Compliance mode SAST must pass), signs artifacts, and emits metrics.

4. `audit-trail.yml` collects metrics and produces an audit report and archive.

Spec Validation
---------------
When a PR touches specs:
- Parse each `.feature` file. For each `Scenario` ensure there is a `Given`, `When`, and `Then` within the scenario block.
- Map each scenario to a requirement ID or path. Fail if any scenario is unmapped.
- Output `reports/spec-coverage.json` with fields:
  - `spec_coverage` (percentage of scenarios that have mappings)
  - `total_scenarios`
  - `implemented_scenarios` (best-effort - scenarios appearing in generated code/tests)
  - `errors` (array of {file, line, issue})

Spec Validation example output (JSON):
```
{
  "spec_coverage": 95,
  "total_scenarios": 20,
  "implemented_scenarios": 19,
  "errors": [
    {"file": "specs/payments.feature", "line": 12, "issue": "Missing Given step"}
  ]
}
```

Code & Test Generation
----------------------
Trigger: only after spec validation succeeds (or in Sandbox mode when explicitly allowed).

Rules:
- For each `Feature` produce a core domain module under `src/core/<feature_name>`.
- Produce adapter code under `src/adapters` (HTTP controllers, DB gateways, etc.) only for ports specified in the spec.
- Generate unit tests under `tests/unit` and integration tests under `tests/integration` mirroring scenarios.
- Update living documentation under `docs/` mapping `feature -> scenario -> implementation -> tests`.
- Create a single commit on `auto/spec-implementation` and open a PR against `main`.

Generation Checkpoints (must be recorded in JSON + Markdown):
- Spec coverage (% scenarios implemented)
- Tests generated (list)
- Build status (compiled/success)
- Estimated resource usage (CPU milli, memory MiB, storage MiB)
- SAST (CodeQL) quick-pass status

If a generation step cannot meet resource KPIs or fails static checks, stop and report.

CI/CD Enforcement
-----------------
- Linting: `flake8 src tests` (must pass)
- Testing: `pip install -r requirements.txt` then `PYTHONPATH=$PWD pytest --maxfail=1 --disable-warnings -q` (must pass)
- Docker build: `docker build -t example-service:${{ github.sha }} .` (must succeed)
- SBOM: generate via cyclonedx-bom and produce `sbom.xml` — then sign artifacts with `cosign` (must be present)
- SAST/CodeQL: run CodeQL. In Sandbox mode SAST may be optional; in Compliance mode a non-clean SAST must block merge.
- Audit: log results into `audit-reports/YYYY-MM-DD/<run_id>.json`

Audit Report (minimum fields)
```
{
  "run_id": "2025-09-17T12:00Z",
  "workflow": "CI Pipeline",
  "status": "failed",
  "spec_coverage": 92,
  "test_results": {"unit": "45/47 passed", "integration": "12/12 passed"},
  "resources": {"cpu": "150m", "memory": "220Mi", "storage": "15Mi"},
  "security": {"sast": "clean", "dependencies": ["requests 2.31 vulnerable -> upgrade"]}
}
```

Behavior Rules (Hard Constraints)
--------------------------------
- Never generate code that is not traceable to a spec. If the spec lacks detail, stop and request clarification.
- Never accept direct human code changes to `src/` or `tests/` that are not produced by the auto-generation pipeline. Human commits that touch `src/` or `tests/` should be flagged and rejected in Compliance mode.
- Enforce hexagonal architecture: domain logic only in `src/core`, adapters in `src/adapters`, infra in `src/infra`.
- Always update living documentation when code is generated.

Spec-to-Code Concrete Example
-----------------------------
Input `features/ping.feature`:
```gherkin
Feature: Ping endpoint
  In order to check if the service is alive
  As a system monitor
  I want a /ping endpoint that returns "pong"

  Scenario: Service responds to ping
    Given the service is running
    When I call GET /ping
    Then I receive a 200 OK
    And the body contains "pong"
```

Expected generated files (example):

1) `src/core/ping/service.py`
```python
class PingService:
    def check(self) -> str:
        return "pong"
```

2) `src/adapters/http/ping_controller.py`
```python
from fastapi import APIRouter
from src.core.ping.service import PingService

router = APIRouter()

@router.get("/ping")
def ping():
    service = PingService()
    return {"message": service.check()}
```

3) `src/main.py` (if missing): include router
```python
from fastapi import FastAPI
from src.adapters.http.ping_controller import router as ping_router

app = FastAPI()
app.include_router(ping_router)
```

4) `tests/unit/test_ping_service.py`
```python
from src.core.ping.service import PingService

def test_ping_service_returns_pong():
    service = PingService()
    assert service.check() == "pong"
```

5) `tests/integration/test_ping_endpoint.py`
```python
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_ping_endpoint_returns_pong():
    response = client.get("/ping")
    assert response.status_code == 200
    assert response.json() == {"message": "pong"}
```

6) Living docs: `docs/spec-coverage.md` updated with mapping.

Checkpoints recorded:
- `spec_coverage`: 100
- `unit_tests`: pass
- `integration_tests`: pass
- `sbom`: generated & signed

Practical Enforcement Examples
-----------------------------
- If `validate-specs.yml` detects a missing `Given`, it must fail with a JSON report in `reports/spec-coverage.json` and block merging.
- `auto-generate.yml` must open a PR from `auto/spec-implementation` with only generated code and a clear summary of mappings and checkpoints.
- `ci.yml` on `auto/spec-implementation` must run the full pipeline; in Compliance mode a failing SAST must stop the merge.

Reporting & Storage
-------------------
- Store audit artifacts under `audit-reports/YYYY-MM-DD/<run_id>.json` and also upload as GitHub Actions artifacts for the run.
- Store spec coverage under `reports/spec-coverage.json` and upload it as an artifact.

Developer Guidance (for admins)
-------------------------------
- Sandbox mode: enable `auto-generate.yml` to run but do not enable branch protection requiring its checks.
- Compliance mode: enable branch protection requiring `validate-specs`, `ci` and `audit-trail` checks; disallow direct pushes to `main`.

Local Try-it Commands
---------------------
To run generated tests locally:
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
PYTHONPATH=$PWD pytest tests/unit tests/integration -q
```

To generate SBOM and sign (locally):
```bash
pip install cyclonedx-bom cosign
cyclonedx-py requirements requirements.txt -o sbom.xml
# cosign sign --key cosign.key docker-image:tag
```

Behavioral Failsafe
-------------------
- If a generated run fails any Compliance checkpoint, the system MUST NOT attempt silent fixes. Notify the spec author with the JSON and Markdown reports and await human remediation.

Appendix: Templates and Examples
--------------------------------
- Provide canonical job templates (lint, test, docker-build, sbom-sign, sast) in `.github/workflows/` and ensure `validate-specs.yml`, `auto-generate.yml`, and `audit-trail.yml` are present.

---
End of Spec-to-System Copilot Instructions.
